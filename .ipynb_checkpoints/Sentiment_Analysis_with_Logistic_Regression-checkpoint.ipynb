{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import twitter_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "twitter_samples?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_positive_tweets = twitter_samples.strings('positive_tweets.json')\n",
    "all_negative_tweets = twitter_samples.strings('negative_tweets.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5000"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(all_positive_tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5000"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(all_negative_tweets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Split Data into 80% training set and 20% test set**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_pos = all_positive_tweets[:4000]\n",
    "test_pos = all_positive_tweets[4000:]\n",
    "train_neg = all_negative_tweets[:4000]\n",
    "test_neg = all_negative_tweets[4000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x = train_pos + train_neg\n",
    "test_x = test_pos + test_neg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_y = np.append(np.ones((len(train_pos),1)), np.zeros((len(train_neg),1)), axis=0)\n",
    "test_y = np.append(np.ones((len(test_pos),1)), np.zeros((len(test_neg),1)), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2000, 1)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Preprocess Tweets\n",
    "1. Remove URLs, twitter marks and styles\n",
    "2. Tokenize and Lowercase\n",
    "3. Remove stopwords and punctuation\n",
    "4. Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'My beautiful sunflowers on a sunny Friday morning off :) #sunflowers #favourites #happy #Friday off… https://t.co/3tfYom0N1i'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweet = all_positive_tweets[2277]\n",
    "tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import string\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import TweetTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_tweet(tweet):\n",
    "    ''' Process Tweet Function\n",
    "    Input:\n",
    "        tweet: a string containing a tweet\n",
    "    Output:\n",
    "        A list of words containing preprocessed tweet\n",
    "    '''\n",
    "    stopwords_english = stopwords.words('english')\n",
    "    stemmer = PorterStemmer()\n",
    "    \n",
    "    # remove old style retweet text RT\n",
    "    tweet = re.sub(r'^RT[\\s]+', '', tweet)\n",
    "\n",
    "    # remove hyperlinks\n",
    "    tweet = re.sub(r'https?:\\/\\/.*[\\r\\n]*', '', tweet)\n",
    "\n",
    "    # remove hashtags\n",
    "    tweet = re.sub(r'#', '', tweet)\n",
    "    \n",
    "    # tokenize the string: split the strings into individual words without blanks or tabs\n",
    "    tokenizer = TweetTokenizer(preserve_case=False, reduce_len=True, strip_handles=True)\n",
    "    tweet_tokens = tokenizer.tokenize(tweet)\n",
    "    \n",
    "    # Removing stopwords and punctuation and Stemming  \n",
    "    tweet_stem = []\n",
    "\n",
    "    for word in tweet_tokens:\n",
    "        if word not in stopwords_english and word not in string.punctuation:\n",
    "#             tweet_clean.append(word)\n",
    "            stem_word = stemmer.stem(word)\n",
    "            tweet_stem.append(stem_word)\n",
    "    \n",
    "    return tweet_stem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['beauti', 'sunflow', 'sunni', 'friday', 'morn', ':)', 'sunflow', 'favourit', 'happi', 'friday', '…']\n"
     ]
    }
   ],
   "source": [
    "print(process_tweet(tweet))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Build Frequencies\n",
    "* Build vocabulory dictionary from training data in the form of {(word,label):freq}. \n",
    "* train_x & train_y is the corpus of tweets that is used to build frequency dictionary\n",
    "* Vocabulory is the set of unique words from corpus and its positive frequency is the number of times that word has appeared in positive tweets and negative frequency is the number of times that word has appeared in negative tweets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_freqs(tweets, labels):\n",
    "    ''' Build Frequencies\n",
    "    Input:\n",
    "        tweets: A list of tweets\n",
    "        labels: An mx1 array with the sentiment label of each tweet (1 or 0)\n",
    "    Output:\n",
    "        freqs: A dictionary mapping each (word,sentiment) pair to its frequency\n",
    "    '''\n",
    "    \n",
    "    labels_list = np.squeeze(labels).tolist()\n",
    "    \n",
    "    freqs = {} # empty dictionary\n",
    "    \n",
    "    for label,tweet in zip(labels_list,tweets):\n",
    "        for word in process_tweet(tweet):\n",
    "            pair = (word,label)\n",
    "            if pair in freqs:\n",
    "                freqs[pair] += 1\n",
    "            else:\n",
    "                freqs[pair] = 1\n",
    "    \n",
    "    return freqs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build frequency dictionary\n",
    "freqs = build_freqs(train_x, train_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Extract Features\n",
    "* Features are extracted from frequency dictionary.\n",
    "* Feature of tweet m: X_m = [1, sum of positive frequencies, sum of negative frequencies]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features(tweet, freqs):\n",
    "    '''\n",
    "    Input:\n",
    "        tweet: raw tweet without any processing\n",
    "        freqs: frequency dictionary (word,label):frequency\n",
    "    Output:\n",
    "        feature = [1, sum of positive frequencies, sum of negative frequencies]\n",
    "    '''\n",
    "    # process tweet\n",
    "    words = process_tweet(tweet)\n",
    "    \n",
    "    # initialize 1X3 vector to populate with features later\n",
    "    x = np.zeros((1,3))\n",
    "    \n",
    "    # bias term = 1\n",
    "    x[0,0] = 1\n",
    "    \n",
    "    # calculate sum of positive frequencies and sum of negative frequencies\n",
    "    for word in words:\n",
    "        if (word,1.0) in freqs.keys():\n",
    "            x[0,1] += freqs[(word,1.0)]\n",
    "        if (word,0.0) in freqs.keys():\n",
    "            x[0,2] += freqs[(word,0.0)]\n",
    "    \n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract features from training data\n",
    "X = np.zeros((len(train_x), 3))\n",
    "for i in range(len(train_x)):\n",
    "    X[i,:] = extract_features(train_x[i], freqs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Train Logistic Regression Model\n",
    "* Write gradient descent function to minimize cost of training\n",
    "* Call `Gradient Descent` function on training features X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent(x, y, theta, alpha, num_iteration):\n",
    "    '''\n",
    "    Input:\n",
    "        x: matrix of features\n",
    "        y: labels for feature matrix\n",
    "        theta: weight vector\n",
    "        alpha: learning rate\n",
    "        num_iteration: number of iterations\n",
    "    Output:\n",
    "        J: final cost\n",
    "        theta: final weight vectore\n",
    "    '''\n",
    "    \n",
    "    m = len(x) #number of rows\n",
    "    \n",
    "    for i in range(0,num_iteration):\n",
    "        # call sigmoid function\n",
    "        z = np.dot(x,theta)\n",
    "        h = 1/(1 + np.exp(-z))\n",
    "        \n",
    "        # calculate cost function\n",
    "        J = (-1/float(m))*(np.dot(np.transpose(y), np.log(h)) + np.dot(np.transpose(1-y), np.log(1-h)))\n",
    "        \n",
    "        # update theta\n",
    "        theta = theta - (alpha/m)*np.dot(np.transpose(x), (h-y))\n",
    "        \n",
    "    J = float(J)\n",
    "    return J,theta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "J,theta = gradient_descent(X, train_y, np.zeros((3,1)), 1e-9, 1500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.24216576985691673"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "J"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 7.25263662e-08],\n",
       "       [ 5.23898548e-04],\n",
       "       [-5.55169894e-04]])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "theta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Test Logistic Regression Model\n",
    "* write predict_tweet() function to predict the sentiment of tweet\n",
    "* write test_logistic_regression function to evaluate the model on test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_tweet(tweet, freqs, theta):\n",
    "    '''\n",
    "    Input:\n",
    "        tweet: a string\n",
    "        freqs: frequency dictionary\n",
    "        theta: weight vector\n",
    "    Output:\n",
    "        y_predict: probability of tweet being positive or negative\n",
    "    '''\n",
    "    # extract features of tweet\n",
    "    x = extract_features(tweet, freqs)\n",
    "    \n",
    "    # predict sentiment using updated weight vector from gradient_descent function\n",
    "    z = np.dot(x,theta)\n",
    "    y_predict = 1/(1 + np.exp(-z))\n",
    "        \n",
    "    return y_predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This movie is good -> [[0.51336031]]\n",
      "I am happy -> [[0.51858011]]\n",
      "I am sad -> [[0.48677873]]\n",
      "This movie is bad -> [[0.49420623]]\n"
     ]
    }
   ],
   "source": [
    "tweets = ['This movie is good','I am happy','I am sad','This movie is bad']\n",
    "for tweet in tweets:\n",
    "    print('{} -> {}'.format(tweet, predict_tweet(tweet,freqs,theta)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_logistic_regression(test_x, test_y, freqs, theta):\n",
    "    '''\n",
    "    Input:\n",
    "        test_x: list of tweets\n",
    "        test_y: corresponding labels for list of tweets\n",
    "        freqs: frequency dictionary\n",
    "        theta: weight vector    \n",
    "    Output:\n",
    "        accuracy: number of tweets classified correctly/ total number of tweets\n",
    "    '''\n",
    "    \n",
    "    y_hat = [] #list for storing predictions\n",
    "    \n",
    "    for tweet in test_x:\n",
    "        y_predict = predict_tweet(tweet, freqs, theta)\n",
    "        \n",
    "        if y_predict > 0.5:\n",
    "            y_hat.append(1.0)\n",
    "        else:\n",
    "            y_hat.append(0)\n",
    "            \n",
    "    # Calculate Accuracy\n",
    "    # y_hat: a list  test_y:m,1 array convert it to m, dimension for comparison\n",
    "    test_y = np.squeeze(test_y)\n",
    "    \n",
    "    accuracy = sum(y_hat == test_y)/len(test_y)\n",
    "    \n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.995"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_logistic_regression(test_x, test_y, freqs, theta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
