{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import twitter_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "twitter_samples?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_positive_tweets = twitter_samples.strings('positive_tweets.json')\n",
    "all_negative_tweets = twitter_samples.strings('negative_tweets.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5000"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(all_positive_tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5000"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(all_negative_tweets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split Data into 80% training set and 20% test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_pos = all_positive_tweets[:4000]\n",
    "test_pos = all_positive_tweets[4000:]\n",
    "train_neg = all_negative_tweets[:4000]\n",
    "test_neg = all_negative_tweets[4000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x = train_pos + train_neg\n",
    "test_x = test_pos + test_neg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_y = np.append(np.ones((len(train_pos),1)), np.zeros((len(train_neg),0)), axis=1)\n",
    "test_y = np.append(np.ones((len(test_pos),1)), np.zeros((len(test_neg),0)), axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess Tweets\n",
    "1. Remove URLs, twitter marks and styles\n",
    "2. Tokenize and Lowercase\n",
    "3. Remove stopwords and punctuation\n",
    "4. Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'My beautiful sunflowers on a sunny Friday morning off :) #sunflowers #favourites #happy #Friday off… https://t.co/3tfYom0N1i'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweet = all_positive_tweets[2277]\n",
    "tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import string\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import TweetTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_tweet(tweet):\n",
    "    ''' Process Tweet Function\n",
    "    Input:\n",
    "        tweet: a string containing a tweet\n",
    "    Output:\n",
    "        A list of words containing preprocessed tweet\n",
    "    '''\n",
    "    stopwords_english = stopwords.words('english')\n",
    "    stemmer = PorterStemmer()\n",
    "    \n",
    "    # remove old style retweet text RT\n",
    "    tweet = re.sub(r'^RT[\\s]+', '', tweet)\n",
    "\n",
    "    # remove hyperlinks\n",
    "    tweet = re.sub(r'https?:\\/\\/.*[\\r\\n]*', '', tweet)\n",
    "\n",
    "    # remove hashtags\n",
    "    tweet = re.sub(r'#', '', tweet)\n",
    "    \n",
    "    # tokenize the string: split the strings into individual words without blanks or tabs\n",
    "    tokenizer = TweetTokenizer(preserve_case=False, reduce_len=True, strip_handles=True)\n",
    "    tweet_tokens = tokenizer.tokenize(tweet)\n",
    "    \n",
    "    # Removing stopwords and punctuation and Stemming  \n",
    "    tweet_stem = []\n",
    "\n",
    "    for word in tweet_tokens:\n",
    "        if word not in stopwords_english and word not in string.punctuation:\n",
    "#             tweet_clean.append(word)\n",
    "            stem_word = stemmer.stem(word)\n",
    "            tweet_stem.append(stem_word)\n",
    "    \n",
    "    return tweet_stem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['beauti', 'sunflow', 'sunni', 'friday', 'morn', ':)', 'sunflow', 'favourit', 'happi', 'friday', '…']\n"
     ]
    }
   ],
   "source": [
    "print(process_tweet(tweet))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build Frequencies\n",
    "* Build vocabulory dictionary from training data in the form of {(word,label):freq}. \n",
    "* train_x & train_y is the corpus of tweets that is used to build frequency dictionary\n",
    "* Vocabulory is the set of unique words from corpus and its positive frequency is the number of times that word has appeared in positive tweets and negative frequency is the number of times that word has appeared in negative tweets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_freqs(tweets, labels):\n",
    "    ''' Build Frequencies\n",
    "    Input:\n",
    "        tweets: A list of tweets\n",
    "        labels: An mx1 array with the sentiment label of each tweet (1 or 0)\n",
    "    Output:\n",
    "        freqs: A dictionary mapping each (word,sentiment) pair to its frequency\n",
    "    '''\n",
    "    \n",
    "    labels_list = np.squeeze(labels).tolist()\n",
    "    \n",
    "    freqs = {} # empty dictionary\n",
    "    \n",
    "    for label,tweet in zip(labels_list,tweets):\n",
    "        for word in process_tweet(tweet):\n",
    "            pair = (word,label)\n",
    "            if pair in freqs:\n",
    "                freqs[pair] += 1\n",
    "            else:\n",
    "                freqs[pair] = 1\n",
    "    \n",
    "    return freqs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets = ['This movie is good','That movie is bad']\n",
    "labels = [1,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{('movi', 1): 1, ('good', 1): 1, ('movi', 0): 1, ('bad', 0): 1}"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "build_freqs(tweets,labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
